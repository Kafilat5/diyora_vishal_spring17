{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import glob\n",
    "import csv\n",
    "from collections import Counter\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from collections import OrderedDict\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='vishal6557', api_key='sNc3vqMIVzo5DUhD84pI'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis 1\n",
    "# Saving the article from archive 2016 and 2017 and compare its article \n",
    "# SAVE ALL ARTICLE FROM 2016 and 2017 ARCHIVE FILE to csv file\n",
    "Read article from the all 2016 archive json file.\n",
    "\n",
    "Saving article in all_2016_business.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "source12=[]\n",
    "types_news=[]\n",
    "for filename in glob.iglob('Data/ALL_JSON2016archive.*.json'): #iterating through each file of 2016 archive file 22 files(16.2 MB each)\n",
    "    with open(filename) as data_file:      #iterating through each file\n",
    "        data = json.load(data_file)   \n",
    "        if len(data[\"response\"][\"docs\"]) >= 1:  \n",
    "            # open the CSV for appending\n",
    "            with open(\"all_2016_archive.csv\", 'w') as csvfile: #saving the FILE AS all_2016\n",
    "                fieldnames=['Publish_Dates','Mains','Sources','Document_Types','Web_Urls','New_Desks','Section_Names','Snippets']\n",
    "                out_file = csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "                out_file.writeheader()\n",
    "                for article in data['response']['docs']:\n",
    "#                     if article['news_desk'] =='Business': #If article contains business \n",
    "                    pub_dates,section_names,snippets,lead_paragraphs,mains,sources,document_types,web_urls,sub_section = [],[],[],[],[],[],[],[],[]\n",
    "                    pub_dates=article[\"pub_date\"][:10]\n",
    "                    mains=str(article[\"headline\"][\"main\"]) if \"main\" in article[\"headline\"].keys() else \"\" \n",
    "                    sources=str(article[\"source\"]) if \"source\" in article.keys() else \"\"\n",
    "                    \n",
    "                    document_types=str(article[\"document_type\"]) if \"document_type\" in article.keys() else \"\"\n",
    "                    source12.append(document_types)\n",
    "                    web_urls=article[\"web_url\"] if \"web_url\" in article.keys() else \"\"\n",
    "                    new_desks= str(article[\"news_desk\"]) if \"news_desk\" in article.keys() else \"\"\n",
    "                    types_news.append( new_desks)\n",
    "                    section_names=str(article[\"section_name\"]) if \"section_name\" in article.keys() else \"\"\n",
    "                    snippets=str(article[\"snippet\"]) if \"snippet\" in article.keys() else \"\"\n",
    "                    out_file.writerow({'Publish_Dates':pub_dates,'Mains':mains,'Sources':sources,'Document_Types':document_types,'Web_Urls':web_urls,'New_Desks':new_desks,'Section_Names':section_names,'Snippets':snippets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_section2016=Counter(types_news)\n",
    "sub_section2016=OrderedDict(sorted(sub_section2016.items(), key=lambda pair: pair[1], reverse=True))\n",
    "# sub_section2016.values()\n",
    "\n",
    "# list(sub_section2016)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "snippet=[]\n",
    "sub_section1=[]\n",
    "section=[]\n",
    "types_news1=[]\n",
    "for filename in glob.iglob('Data/ALL_JSON2017Archive.*.json'): #iterating through each file of 2016 archive file 22 files(16.2 MB each)\n",
    "    with open(filename) as data_file:      #iterating through each file\n",
    "        data = json.load(data_file)   \n",
    "        if len(data[\"response\"][\"docs\"]) >= 1:  \n",
    "            # open the CSV for appending\n",
    "            with open(\"all_2017_archive.csv\", 'w') as csvfile: #saving the \n",
    "                fieldnames=['Publish_Dates','Mains','Sources','Document_Types','Web_Urls','New_Desks','Section_Names','Snippets']\n",
    "                out_file = csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "                out_file.writeheader()\n",
    "                for article in data['response']['docs']:\n",
    "#                     if article['news_desk'] =='Business': #If article contains business \n",
    "                    pub_dates,section_names,snippets,lead_paragraphs,mains,sources,document_types,web_urls,sub_section = [],[],[],[],[],[],[],[],[]\n",
    "                    sub_sections=article[\"subsection_name\"] #Find the subsection\n",
    "                    sub_section1.append(sub_sections)\n",
    "                    pub_dates=article[\"pub_date\"][:10]\n",
    "                    mains=str(article[\"headline\"][\"main\"]) if \"main\" in article[\"headline\"].keys() else \"\" \n",
    "                    sources=str(article[\"source\"]) if \"source\" in article.keys() else \"\"\n",
    "                    document_types=str(article[\"document_type\"]) if \"document_type\" in article.keys() else \"\"\n",
    "                    web_urls=article[\"web_url\"] if \"web_url\" in article.keys() else \"\"\n",
    "                    new_desks= str(article[\"news_desk\"]) if \"news_desk\" in article.keys() else \"\"\n",
    "                    types_news1.append(new_desks)\n",
    "                    section_names=str(article[\"section_name\"]) if \"section_name\" in article.keys() else \"\"\n",
    "                    snippets=str(article[\"snippet\"]) if \"snippet\" in article.keys() else \"\"\n",
    "                    out_file.writerow({'Publish_Dates':pub_dates,'Mains':mains,'Sources':sources,'Document_Types':document_types,'Web_Urls':web_urls,'New_Desks':new_desks,'Section_Names':section_names,'Snippets':snippets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['National', 'Foreign', 'Business', 'Culture', 'Sports', 'OpEd', 'Metro', 'Styles', 'Weekend', 'Letters', 'BookReview', 'Editorial', 'Science', \"Fashion & Style / Women's Runway\", 'Learning', 'NYTNow', 'Dining', 'Arts&Leisure', 'TStyle', 'Travel', 'Magazine', 'RealEstate', 'U.S. / Politics', 'SundayBusiness', 'None', 'Society', 'Arts / Music', 'Summary', 'Games', 'Metropolitan', 'T Magazine / Fashion & Beauty', 'Fashion & Style', 'Real Estate', 'U.S.', 'Watching', 'Insider', 'Movies', 'Style', 'Upshot', 'Podcasts', 'Blogs', 'Well', 'Corrections', 'T Magazine', 'Food', 'Fashion & Style / Weddings', \"Fashion & Style / Men's Runway\", 'SpecialSections', 'N.Y. / Region', 'EdLife', 'Business Day', 'Opinion', 'T Magazine / Art', 'World', 'Arts', 'Books', 'World / Europe', 'Times Insider', 'World / Americas', 'World / Asia Pacific', 'Technology', 'Business Day / DealBook', 'Obituaries', 'Multimedia/Photos', 'T Magazine / Entertainment', 'Automobiles', 'The Upshot', 'Well / Live', 'World / Middle East', 'Fashion & Style / Men’s Style', 'Education / Education Life', 'Crosswords & Games', 'Sports / Pro Football', 'Automobiles / New Cars', 'Arts / Art & Design', 'Job Market', 'Universal', 'Your Money', 'Science / Space & Cosmos', 'Universal / América', 'Investigative', 'World / Africa', 'Science / Environment', 'Sports / College Basketball', 'Politics', 'Health', 'Washington', 'The Learning Network', 'Real Estate / Commercial Real Estate', 'International Home', 'Arts / Television', 'Books / Book Review'])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_section2017=Counter(types_news1)\n",
    "\n",
    "# sub_section2017=sorted(sub_section2017.items(), key=lambda pair: pair[1], reverse=True)\n",
    "sub_section2017=OrderedDict(sorted(sub_section2017.items(), key=lambda pair: pair[1], reverse=True))\n",
    "sub_section2017.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~vishal6557/54.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = {\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"values\": list(sub_section2016.values())[:10],\n",
    "      \"labels\": list(sub_section2016.keys())[:10],\n",
    "      \"domain\": {\"x\": [0, .50]},\n",
    "      \"name\": \"2016 Business Subsection\",\n",
    "      \"hoverinfo\":\"label+percent+name\",\n",
    "      \"hole\": .4,\n",
    "      \"type\": \"pie\"\n",
    "    },     \n",
    "    {\n",
    "      \"values\": list(sub_section2017.values())[:10],\n",
    "      \"labels\": list(sub_section2017.keys())[:10],\n",
    "      \"text\":\"2017_Archive\",\n",
    "      \"textposition\":\"inside\",\n",
    "      \"domain\": {\"x\": [.50, 1]},\n",
    "      \"name\": \"2017\",\n",
    "      \"hoverinfo\":\"label+percent+name\",\n",
    "      \"hole\": .4,\n",
    "      \"type\": \"pie\"\n",
    "    }],\n",
    "  \"layout\": {\n",
    "        \"title\":\"Compare Articles number in Archive API between 2016 and 2017\",\n",
    "        \"annotations\": [\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 20\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"2016\",\n",
    "                \"x\": 0.20,\n",
    "                \"y\": 0.5\n",
    "            },\n",
    "            {\n",
    "                \"font\": {\n",
    "                    \"size\": 20\n",
    "                },\n",
    "                \"showarrow\": False,\n",
    "                \"text\": \"2017\",\n",
    "                \"x\": 0.8,\n",
    "                \"y\": 0.5\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "sub_section1=[]\n",
    "for filename in glob.iglob('Data/ALL_JSON.*.json'): #iterating through each file of 2016 archive file 22 files(16.2 MB each)\n",
    "    with open(filename) as data_file:      #iterating through each file\n",
    "        data = json.load(data_file)   \n",
    "        if len(data[\"response\"][\"docs\"]) >= 1:  \n",
    "            # open the CSV for appending\n",
    "            with open(\"all_2017_job_posts.csv\", 'w') as csvfile: #saving the \n",
    "                fieldnames=['Publish_Dates','Mains','Sources','Document_Types','Web_Urls','New_Desks','Section_Names','Snippets']\n",
    "                out_file = csv.DictWriter(csvfile,fieldnames=fieldnames)\n",
    "                out_file.writeheader()\n",
    "                for article in data['response']['docs']:\n",
    "                    if article['news_desk'] =='DealBook': #If article contains business \n",
    "                        pub_dates,section_names,snippets,lead_paragraphs,mains,sources,document_types,web_urls,sub_section = [],[],[],[],[],[],[],[],[]\n",
    "                        sub_sections=article[\"subsection\"] #Find the subsection\n",
    "                        sub_section1.append(sub_sections)\n",
    "                        pub_dates=article[\"pub_date\"][:10]\n",
    "                        mains=str(article[\"headline\"][\"main\"]) if \"main\" in article[\"headline\"].keys() else \"\" \n",
    "                        sources=str(article[\"source\"]) if \"source\" in article.keys() else \"\"\n",
    "                        source.append\n",
    "                        document_types=str(article[\"document_type\"]) if \"document_type\" in article.keys() else \"\"\n",
    "                        web_urls=article[\"web_url\"] if \"web_url\" in article.keys() else \"\"\n",
    "                        new_desks= str(article[\"news_desk\"]) if \"news_desk\" in article.keys() else \"\"\n",
    "#                         types_news1.append(new_desks)\n",
    "                        section_names=str(article[\"section_name\"]) if \"section_name\" in article.keys() else \"\"\n",
    "                        snippets=str(article[\"snippet\"]) if \"snippet\" in article.keys() else \"\"\n",
    "                        out_file.writerow({'Publish_Dates':pub_dates,'Mains':mains,'Sources':sources,'Document_Types':document_types,'Web_Urls':web_urls,'New_Desks':new_desks,'Section_Names':section_names,'Snippets':snippets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
